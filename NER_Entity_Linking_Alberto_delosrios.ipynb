{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Named Entity Recognition and Entity Linking Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:  \n",
    "Alberto de los Ríos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is recommended to start with general import statements\n",
    "from utility_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should load the raw dataset for the task.  \n",
    "Remember to use relative paths to load any files in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1177 = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\")\n",
    "\n",
    "#df_lt = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\")\n",
    "\n",
    "#df_wiki = pd.read_parquet(\"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: LLMs for NER Survey\n",
    "\n",
    "This survey will be evalutated based on the following metrics and dimensions:\n",
    "\n",
    "- Language Capability (Swedish proficiency)\n",
    "- Biomedical Knowledge (Domain relevance)\n",
    "- Computational Efficiency (Training/inference costs)\n",
    "- Performance Metrics (NER-specific scores)\n",
    "- Ontology Compatibility (Linking to ICD/ICF/LOINC)\n",
    "\n",
    "PONER UN POCO DE HISTORIA DE BERT creado por los researches the Google\n",
    "\n",
    "For this first task we will only use the 1177 Vårdguiden dataset since it is the lightest with only 927 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM in Swedish\n",
    "\n",
    "Advantages:\n",
    "1. Authenticity - Maintains original clinical nuance and terminology\n",
    "2. No translation errors - Avoids introducing errors from machine translation\n",
    "3. Proper noun handling - Swedish patient/place names and untranslatable terms remain intact\n",
    "4. Future applicability - Model will work natively with Swedish EHR systems\n",
    "5. Matching Swedish ontologies - Direct alignment with Swedish medical coding systems\n",
    "\n",
    "Limitations:\n",
    "1. Limited models - Fewer Swedish biomedical LLMs available:\n",
    "- KB/bert-base-swedish-cased: [Swedish BERT models for NER](https://huggingface.co/KB/bert-base-swedish-cased)\n",
    "- [RoBERTa large](https://huggingface.co/AI-Sweden-Models/roberta-large-1160k)\n",
    "- Swedish GPT models with limited NER capacity: [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
    "2. Smaller datasets - The Swedish medical NER dataset has only ~6,000 annotated entries\n",
    "3. Debugging difficulty - Hard to verify annotations/errors without Swedish knowledge\n",
    "4. Resource scarcity - Few Swedish stopword lists, tokenizers, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM in English\n",
    "\n",
    "- [Swedish BERT models for NER](https://huggingface.co/KB/bert-base-swedish-cased)\n",
    "- [RoBERTa large](https://huggingface.co/AI-Sweden-Models/roberta-large-1160k)\n",
    "- [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
    "\n",
    "\n",
    "Advantages:\n",
    "1. Model availability - Access powerful English biomedical models:\n",
    "- [BioBERT](https://github.com/naver/biobert-pretrained?tab=readme-ov-file)\n",
    "- [ClinicalBERT](https://huggingface.co/medicalai/ClinicalBERT)\n",
    "- [now BiomedBERT, previously known as PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext)\n",
    "2. Larger datasets - Can augment with English medical NER datasets (~20+ available)\n",
    "3. Easier debugging - You can understand the text for error analysis\n",
    "4. More tutorials - Abundant English NLP examples\n",
    "5. Ontology linking - English ontologies (ICD-10 English) have more community support\n",
    "\n",
    "Limitations:\n",
    "1. Translation errors - Clinical terms often mistranslated:\n",
    "2. Back-translation complexity - Need to map English predictions back to Swedish text\n",
    "3. Loss of context - Swedish compound words get split unnaturally\n",
    "4. Ontology mismatch - Swedish medical codes don't align perfectly with English\n",
    "Added pipeline complexity - Requires translation component\n",
    "\n",
    "- [BioBERT vs PubMedBERT](https://medium.com/@EleventhHourEnthusiast/model-comparison-biobert-vs-pubmedbert-8c2d78178d10)\n",
    "- [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual LLM\n",
    "\n",
    "- [mBERT](https://huggingface.co/google-bert/bert-base-multilingual-cased), 2018 \n",
    "- [XLM-RoBERTa](https://huggingface.co/FacebookAI/xlm-roberta-base), 2019\n",
    "- [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
    "\n",
    "\n",
    "Advantages:\n",
    "1. Cross-Lingual Knowledge Transfer: Leverages patterns from high-resource languages (e.g., German medical terms help Swedish). Fine-tune on Swedish data but benefit from pretraining on multilingual medical corpora.\n",
    "2. Handling Code-Switching: No need for manual language detection.\n",
    "3. Robust Tokenization: SentencePiece (used in XLM-R) handles Swedish compounds better than WordPiece (mBERT):\n",
    "4. Future-Proofing: One model can support other languages (e.g., adding Norwegian EHRs later).\n",
    "\n",
    "Limitations:\n",
    "1. Translation errors - Clinical terms often mistranslated:\n",
    "2. Back-translation complexity - Need to map English predictions back to Swedish text\n",
    "3. Loss of context - Swedish compound words get split unnaturally\n",
    "4. Ontology mismatch - Swedish medical codes don't align perfectly with English\n",
    "Added pipeline complexity - Requires translation component\n",
    "\n",
    "- [BioBERT vs PubMedBERT](https://medium.com/@EleventhHourEnthusiast/model-comparison-biobert-vs-pubmedbert-8c2d78178d10)\n",
    "- [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is mandatory to maintain the headings for each task.  \n",
    "OPTIONALLY, you can use one level down (###) to organize subsessions of the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use markdown cells like this one to include:\n",
    "- Discussion points.\n",
    "- References to specific sources of code that you might have used to solve the assignment.\n",
    "- General commentas and explanations about your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: This is the title of task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain the solution of task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain:\n",
    "- Results.\n",
    "- Summary of best model performance:\n",
    "    - Name of best model file as saved in /models.\n",
    "    - Relevant scores such as: accuracy, precision, recall, F1-score, etc.\n",
    "- Key discussion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER_Entity_Linking_Alberto_delosrios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Named Entity Recognition and Entity Linking Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:  \n",
    "Alberto de los Ríos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is recommended to start with general import statements\n",
    "#from utility_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should load the raw dataset for the task.  \n",
    "Remember to use relative paths to load any files in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1177 = \"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\"\n",
    "url_lt = \"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\"\n",
    "url_wiki = \"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\"\n",
    "\n",
    "df_1177 = pd.read_parquet(url_1177)\n",
    "df_lt = pd.read_parquet(url_lt)\n",
    "df_wiki = pd.read_parquet(url_wiki)\n",
    "\n",
    "os.makedirs(\"raw_data\", exist_ok=True)\n",
    "df_1177.to_parquet(\"raw_data/1177_train.parquet\", engine=\"pyarrow\")\n",
    "df_lt.to_parquet(\"raw_data/lt_train.parquet\", engine=\"pyarrow\")\n",
    "df_wiki.to_parquet(\"raw_data/wiki_train.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: LLMs for NER Survey\n",
    "\n",
    "This survey will be evalutated based on the following metrics and dimensions:\n",
    "\n",
    "- Language Capability (Swedish proficiency)\n",
    "- Biomedical Knowledge (Domain relevance)\n",
    "- Computational Efficiency (Training/inference costs)\n",
    "- Performance Metrics (NER-specific scores)\n",
    "- Ontology Compatibility (Linking to ICD/ICF/LOINC)\n",
    "\n",
    "PONER UN POCO DE HISTORIA DE BERT creado por los researches the Google\n",
    "\n",
    "For this first task we will only use the 1177 Vårdguiden dataset since it is the lightest with only 927 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM in Swedish\n",
    "\n",
    "Advantages:\n",
    "1. Authenticity - Maintains original clinical nuance and terminology\n",
    "2. No translation errors - Avoids introducing errors from machine translation\n",
    "3. Proper noun handling - Swedish patient/place names and untranslatable terms remain intact\n",
    "4. Future applicability - Model will work natively with Swedish EHR systems\n",
    "5. Matching Swedish ontologies - Direct alignment with Swedish medical coding systems\n",
    "\n",
    "Limitations:\n",
    "1. Limited models - Fewer Swedish biomedical LLMs available:\n",
    "- KB/bert-base-swedish-cased: [Swedish BERT models for NER](https://huggingface.co/KB/bert-base-swedish-cased)\n",
    "- [RoBERTa large](https://huggingface.co/AI-Sweden-Models/roberta-large-1160k)\n",
    "- Swedish GPT models with limited NER capacity: [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
    "2. Smaller datasets - The Swedish medical NER dataset has only ~6,000 annotated entries\n",
    "3. Debugging difficulty - Hard to verify annotations/errors without Swedish knowledge\n",
    "4. Resource scarcity - Few Swedish stopword lists, tokenizers, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM in English\n",
    "\n",
    "- [Swedish BERT models for NER](https://huggingface.co/KB/bert-base-swedish-cased)\n",
    "- [RoBERTa large](https://huggingface.co/AI-Sweden-Models/roberta-large-1160k)\n",
    "- [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
    "\n",
    "\n",
    "Advantages:\n",
    "1. Model availability - Access powerful English biomedical models:\n",
    "- [BioBERT](https://github.com/naver/biobert-pretrained?tab=readme-ov-file)\n",
    "- [ClinicalBERT](https://huggingface.co/medicalai/ClinicalBERT)\n",
    "- [now BiomedBERT, previously known as PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext)\n",
    "2. Larger datasets - Can augment with English medical NER datasets (~20+ available)\n",
    "3. Easier debugging - You can understand the text for error analysis\n",
    "4. More tutorials - Abundant English NLP examples\n",
    "5. Ontology linking - English ontologies (ICD-10 English) have more community support\n",
    "\n",
    "Limitations:\n",
    "1. Translation errors - Clinical terms often mistranslated:\n",
    "2. Back-translation complexity - Need to map English predictions back to Swedish text\n",
    "3. Loss of context - Swedish compound words get split unnaturally\n",
    "4. Ontology mismatch - Swedish medical codes don't align perfectly with English\n",
    "Added pipeline complexity - Requires translation component\n",
    "\n",
    "- [BioBERT vs PubMedBERT](https://medium.com/@EleventhHourEnthusiast/model-comparison-biobert-vs-pubmedbert-8c2d78178d10)\n",
    "- [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual LLM\n",
    "\n",
    "- [mBERT](https://huggingface.co/google-bert/bert-base-multilingual-cased), 2018\n",
    "- [XLM-RoBERTa](https://huggingface.co/FacebookAI/xlm-roberta-base), 2019\n",
    "\n",
    "\n",
    "Advantages:\n",
    "1. Cross-Lingual Knowledge Transfer: Leverages patterns from high-resource languages (e.g., German medical terms help Swedish). Fine-tune on Swedish data but benefit from pretraining on multilingual medical corpora.\n",
    "2. Handling Code-Switching: No need for manual language detection.\n",
    "3. Robust Tokenization: SentencePiece (used in XLM-R) handles Swedish compounds better than WordPiece (mBERT):\n",
    "4. Future-Proofing: One model can support other languages (e.g., adding Norwegian EHRs later).\n",
    "\n",
    "Limitations:\n",
    "1. Translation errors - Clinical terms often mistranslated:\n",
    "2. Back-translation complexity - Need to map English predictions back to Swedish text\n",
    "3. Loss of context - Swedish compound words get split unnaturally\n",
    "4. Ontology mismatch - Swedish medical codes don't align perfectly with English\n",
    "Added pipeline complexity - Requires translation component\n",
    "\n",
    "- [BioBERT vs PubMedBERT](https://medium.com/@EleventhHourEnthusiast/model-comparison-biobert-vs-pubmedbert-8c2d78178d10)\n",
    "- [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 48720\n",
      "Number of sentences with at least one entity: 48720\n",
      "Entity counts from dataset:\n",
      "Disorder/Finding (type 0): 28387\n",
      "Pharmaceutical Drug (type 1): 22858\n",
      "Body Structure (type 2): 20592\n"
     ]
    }
   ],
   "source": [
    "#print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "def analyse_df(df):\n",
    "\n",
    "  df.columns\n",
    "  print(f\"Total number of sentences: {len(df)}\")\n",
    "\n",
    "  # Count sentences with at least one entity\n",
    "  has_entity = df[\"entities\"].apply(lambda ent: len(ent[\"start\"]) > 0)\n",
    "  print(f\"Number of sentences with at least one entity: {has_entity.sum()}\")\n",
    "\n",
    "  from collections import Counter\n",
    "  # Initialize counter\n",
    "  type_counter = Counter()\n",
    "\n",
    "  # Loop through entities and count each type\n",
    "  for entity in df[\"entities\"]:\n",
    "      if \"type\" in entity:\n",
    "          types = entity[\"type\"]\n",
    "          if isinstance(types, np.ndarray) and types.size > 0:\n",
    "              type_counter.update(types.tolist())\n",
    "\n",
    "  # Print results\n",
    "  print(\"Entity counts from dataset:\")\n",
    "  print(f\"Disorder/Finding (type 0): {type_counter[0]}\")\n",
    "  print(f\"Pharmaceutical Drug (type 1): {type_counter[1]}\")\n",
    "  print(f\"Body Structure (type 2): {type_counter[2]}\")\n",
    "\n",
    "#analyse_df(df_1177)\n",
    "#analyse_df(df_lt)\n",
    "analyse_df(df_wiki)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Max number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tokens: 48\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "\n",
    "def max_num_tokens(df):\n",
    "    max_tokens = 0\n",
    "\n",
    "    for sent in df[\"sentence\"]:\n",
    "        tokens = tokenizer(sent)[\"input_ids\"]\n",
    "        max_tokens = max(max_tokens, len(tokens))\n",
    "\n",
    "    print(\"Maximum number of tokens:\", max_tokens)\n",
    "\n",
    "#max_num_tokens(df_1177)\n",
    "#max_num_tokens(df_lt)\n",
    "max_num_tokens(df_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and aligment of tokens and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "[CLS]           -> O\n",
      "{               -> B-BODY\n",
      "kropp           -> I-BODY\n",
      "}               -> I-BODY\n",
      "beskrivs        -> O\n",
      "i               -> O\n",
      "till            -> O\n",
      "exempel         -> O\n",
      "människok       -> O\n",
      "##roppen        -> O\n",
      ",               -> O\n",
      "anat            -> O\n",
      "##omi           -> O\n",
      "och             -> O\n",
      "f               -> O\n",
      "[SEP]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "\n",
      "Sentence 2:\n",
      "[CLS]           -> O\n",
      "sju             -> O\n",
      "miljoner        -> O\n",
      "år              -> O\n",
      "gammalt         -> O\n",
      "hom             -> O\n",
      "##ini           -> O\n",
      "##nt            -> O\n",
      "{               -> B-BODY\n",
      "kran            -> I-BODY\n",
      "##ium           -> I-BODY\n",
      "}               -> I-BODY\n",
      ",               -> O\n",
      "klassific       -> O\n",
      "##erad          -> O\n",
      "som             -> O\n",
      "[SEP]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "\n",
      "Sentence 3:\n",
      "[CLS]           -> O\n",
      "auto            -> O\n",
      "##som           -> O\n",
      "##er            -> O\n",
      "och             -> O\n",
      "ett             -> O\n",
      "par             -> O\n",
      "köns            -> O\n",
      "##kro           -> O\n",
      "##mos           -> O\n",
      "##omer          -> O\n",
      ".               -> O\n",
      "Varje           -> O\n",
      "{               -> B-BODY\n",
      "krom            -> I-BODY\n",
      "##oso           -> I-BODY\n",
      "##m             -> I-BODY\n",
      "}               -> I-BODY\n",
      "består          -> O\n",
      "[SEP]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n",
      "[PAD]           -> O\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer (multilingual, supports Swedish)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "\n",
    "# BIO label mapping\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-DIS\": 1,\n",
    "    \"I-DIS\": 2,\n",
    "    \"B-DRUG\": 3,\n",
    "    \"I-DRUG\": 4,\n",
    "    \"B-BODY\": 5,\n",
    "    \"I-BODY\": 6\n",
    "}\n",
    "\n",
    "# Map integer entity types to BIO label prefixes\n",
    "type_to_bio_prefix = {\n",
    "    0: \"DIS\",\n",
    "    1: \"DRUG\",\n",
    "    2: \"BODY\"\n",
    "}\n",
    "\n",
    "def convert_to_bio_labels(encoding, entity_spans): #Converts character-level entity spans to BIO-formatted token-level labels.\n",
    "\n",
    "    labels = [label2id[\"O\"]] * len(encoding[\"input_ids\"])\n",
    "\n",
    "    for (ent_start, ent_end, ent_type) in entity_spans:\n",
    "        bio_label = type_to_bio_prefix[ent_type]\n",
    "        first_token = True\n",
    "\n",
    "        for idx, (tok_start, tok_end) in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if tok_start == tok_end == 0:\n",
    "                continue  # Skip special tokens\n",
    "\n",
    "            if tok_end > ent_start and tok_start < ent_end:  # Token overlaps entity\n",
    "                if first_token:\n",
    "                    labels[idx] = label2id[f\"B-{bio_label}\"]\n",
    "                    first_token = False\n",
    "                else:\n",
    "                    labels[idx] = label2id[f\"I-{bio_label}\"]\n",
    "\n",
    "    return labels\n",
    "\n",
    "def process_dataset(df, num_examples):\n",
    "    processed = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        sentence = row[\"sentence\"]\n",
    "        entities = row[\"entities\"]  # expects keys: 'start', 'end', 'type'\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=80 #max_number of tokens for df_1177=72 80 to be safe. check the other df\n",
    "        )\n",
    "\n",
    "        # Prepare entity spans for BIO conversion\n",
    "        entity_spans = list(zip(entities[\"start\"], entities[\"end\"], entities[\"type\"]))\n",
    "        bio_labels = convert_to_bio_labels(encoding, entity_spans)\n",
    "        encoding[\"labels\"] = bio_labels\n",
    "\n",
    "        # We no longer need offset_mapping\n",
    "        encoding.pop(\"offset_mapping\")\n",
    "\n",
    "        if i < num_examples:\n",
    "          print(f\"\\nSentence {i+1}:\")\n",
    "          tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "          for token, label_id in zip(tokens, encoding[\"labels\"]):\n",
    "              label_name = list(label2id.keys())[list(label2id.values()).index(label_id)]\n",
    "              print(f\"{token:15} -> {label_name}\")\n",
    "\n",
    "        processed.append(encoding)\n",
    "\n",
    "    return processed\n",
    "\n",
    "    #print(\"\\nEncoding keys example:\", encoding.keys())\n",
    "\n",
    "\n",
    "#encodings_df_1177 = process_dataset(df_1177, 3)\n",
    "#torch.save(encodings_df_1177, \"processed_data/encodings_df_1177.pt\") #Save the processed data\n",
    "# Add this reverse mapping\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "encodings_df_wiki = process_dataset(df_wiki, 3)\n",
    "torch.save(encodings_df_wiki, \"processed_data/encodings_df_wiki_se.pt\") #Save the processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encodings_df_1177[0][\"input_ids\"]) #Print the keys of the first encoding\n",
    "#print(encodings_df_1177[0][\"attention_mask\"])\n",
    "#print(encodings_df_1177[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is already tokenized and shows the spans intances and types of labels.\n",
    "\n",
    "\n",
    " What you do need to do:\n",
    "1. Tokenize the sentence using a pretrained tokenizer, like BertTokenizerFast, which supports alignment of character offsets to token indices.\n",
    "\n",
    "2. Use tokenizer(..., return_offsets_mapping=True) to map character positions (like 'start' and 'end') to token indices.\n",
    "\n",
    "3. Create a labels list per sentence (same length as the number of tokens) initialized to -100 (special value that tells the model to ignore those positions), and then assign label IDs (0, 1, 2) to the tokens that overlap with entity spans.\n",
    "\n",
    "\n",
    "CUENTA TAMBIEN LOS CARACTERES ESPECIALES ({[ DE CADA CLASE ASIQ VALORAR SI NO CONTARLOS O SI. PERO SON INTICATIVOS DE LA TYPO DE LABEL.\n",
    "\n",
    "doubts:\n",
    "1. Why do all sentences need to be the same length (via padding)?\n",
    "Transformers process input as batches of fixed-size sequences. That’s because:\n",
    "\n",
    "Matrix computations (done on GPU) require tensors of uniform shape.\n",
    "\n",
    "Efficient batching boosts performance and stability during training.\n",
    "\n",
    "2. Explanation of common fields in encoding:\n",
    "- input_ids: The token IDs for your sentence (numbers representing tokens).\n",
    "- token_type_ids: Used for distinguishing sentence segments (mostly for tasks like question answering). Might be all zeros if not applicable.\n",
    "- attention_mask: Indicates which tokens are real (1) and which are padding (0).\n",
    "- offset_mapping: For each token, the start and end character positions in the original sentence (useful for aligning labels).\n",
    "\n",
    "3. Explicación:\n",
    "- entities[\"type\"] contains flat integers like 0, 1, 2, etc.\n",
    "- type_to_bio_prefix maps those integers to entity type names (DIS, DRUG, etc.)\n",
    "- convert_to_bio_labels() uses this mapping to apply B- and I- prefixes to the correct tokens.\n",
    "- Then it converts those prefixes using label2id.\n",
    "\n",
    "\n",
    "PONER EXPLICACIÓN DE BIO LABELS Y DE QUE MUCHOS MODELOS SOLO ACEPTAN ESTE TIPO DE LABELS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE CUSTOM PYTORCH DATASET\n",
    "\n",
    "Why create a Dataset class if you already have encodings?\n",
    "- Integration with PyTorch DataLoader:\n",
    "The Dataset class provides a standardized way for PyTorch to access your data. The DataLoader uses the Dataset to efficiently load data in batches, shuffle it, and handle multi-threaded loading.\n",
    "\n",
    "- Batching & Shuffling:\n",
    "When training a model, you typically don’t want to feed data one example at a time. You want mini-batches (e.g., 16 or 32 samples per batch). The DataLoader uses the Dataset to create batches, and you don’t have to manually slice your encodings.\n",
    "\n",
    "- Lazy Access / Memory Efficiency:\n",
    "The Dataset class lets PyTorch load one sample at a time on demand, rather than keeping everything as tensors in memory at once (especially useful with large datasets).\n",
    "\n",
    "- Transforms / Augmentations:\n",
    "If you want to apply any transformations (like token masking, noise, data augmentation) on the fly, the Dataset class is the place to do it — at retrieval time.\n",
    "\n",
    "- Uniform Interface:\n",
    "The Dataset abstracts the data representation so your training loop just works with the Dataset/DataLoader API without worrying about how data is stored.\n",
    "\n",
    "- [Custom Named Entity Recognition with BERT](https://towardsdatascience.com/custom-named-entity-recognition-with-bert-cf1fd4510804/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[2, 20086, 2380, 17454, 11473, 31, 76, 1377, 38399, 14386, 19, 19041, 15664, 36, 13, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def collate_encodings(processed):\n",
    "    encodings = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    labels = []\n",
    "    for item in processed:\n",
    "        encodings[\"input_ids\"].append(item[\"input_ids\"])\n",
    "        encodings[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "        labels.append(item[\"labels\"])\n",
    "    return encodings, labels\n",
    "\n",
    "#encodings, labels = collate_encodings(encodings_df_1177)\n",
    "encodings, labels = collate_encodings(encodings_df_wiki)\n",
    "\n",
    "print(encodings.keys())\n",
    "print(encodings[\"input_ids\"][0])\n",
    "print(encodings[\"attention_mask\"][0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For each key (input_ids, attention_mask), convert the sequence at idx to tensor\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Also convert labels for that idx to tensor\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate inputs and labels\n",
    "all_input_ids = [enc['input_ids'] for enc in encodings_df_wiki]\n",
    "all_attention_mask = [enc['attention_mask'] for enc in encodings_df_wiki]\n",
    "all_labels = [enc['labels'] for enc in encodings_df_wiki]\n",
    "\n",
    "# Pack inputs into one dictionary for convenience\n",
    "all_encodings = {\n",
    "    'input_ids': all_input_ids,\n",
    "    'attention_mask': all_attention_mask\n",
    "}\n",
    "\n",
    "# Split\n",
    "\n",
    "train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_labels, val_labels = train_test_split(\n",
    "    all_input_ids, all_attention_mask, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-create encoding dicts for train and val\n",
    "train_encodings = {\n",
    "    'input_ids': train_input_ids,\n",
    "    'attention_mask': train_attention_mask\n",
    "}\n",
    "\n",
    "val_encodings = {\n",
    "    'input_ids': val_input_ids,\n",
    "    'attention_mask': val_attention_mask\n",
    "}\n",
    "\n",
    "# Now create datasets\n",
    "train_dataset = NERDataset(train_encodings, train_labels)\n",
    "val_dataset = NERDataset(val_encodings, val_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\beto1\\OneDrive - KTH\\1º\\Applied Machine Learning and Artificial Inteligence\\NER_Entity_Linking_Alberto_delosrios\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\beto1\\.cache\\huggingface\\hub\\models--KB--bert-base-swedish-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForTokenClassification, AutoModel, AutoTokenizer\n",
    "\n",
    "# Number of labels (adjust to your label2id)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "#model = XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-base\",num_labels=num_labels)\n",
    "\n",
    "model = AutoModel.from_pretrained('KB/bert-base-swedish-cased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Where to save model/checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',            # TensorBoard logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",           # Save checkpoint every epoch\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=[]) # We'll define metrics below\n",
    "\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_labels = [\n",
    "        [list(label2id.keys())[list(label2id.values()).index(l)] for l in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [list(label2id.keys())[list(label2id.values()).index(p)] for p in pred_seq]\n",
    "        for pred_seq in predictions\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#trainer.save_model(\"models/fine_tuned_xlm_roberta_ner_wiki_se\")\n",
    "#tokenizer.save_pretrained(\"models/fine_tuned_xlm_roberta_ner_wiki_se\")\n",
    "trainer.save_model(\"models/fine_tuned_bert-base-swedish-cased_ner_wiki_se\")\n",
    "tokenizer.save_pretrained(\"models/fine_tuned_bert-base-swedish-cased_ner_wiki_se\")\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.save_pretrained(\"models/fine_tuned_bert-base-swedish-cased_ner_wiki_se\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Fine Tuned model with new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# 1. Load your fine-tuned model\n",
    "#model_path = \"models/fine_tuned_xlm_roberta_ner_wiki_se\"  # adjust to your actual path\n",
    "model_path = \"models/fine_tuned_bert-base-swedish-cased_ner_wiki_se\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# 2. Load NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "def predict_text(df):# 4. Loop through df_1177\n",
    "    # 3. Prepare results list\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        text_id = row[\"sid\"] if \"sid\" in df.columns else i  # fallback to index\n",
    "        text = row[\"sentence\"]  # adjust if your column has a different name\n",
    "\n",
    "        predictions = ner_pipeline(text)\n",
    "        for pred in predictions:\n",
    "            all_predictions.append({\n",
    "                \"id\": text_id,\n",
    "                \"text\": text,\n",
    "                \"entity\": pred[\"word\"],\n",
    "                \"label\": pred[\"entity_group\"],\n",
    "                \"score\": round(pred[\"score\"], 3),\n",
    "                \"start\": pred[\"start\"],\n",
    "                \"end\": pred[\"end\"]\n",
    "            })\n",
    "\n",
    "    # 5. Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "    # 6. Save to CSV\n",
    "    results_df.to_csv(\"results/ner_predictions_1177_se.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "predict_text(df_1177)\n",
    "print(\"Saved predictions to 'results/ner_predictions_1177_se.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beto1\\AppData\\Local\\Temp\\ipykernel_15252\\4280058965.py:2: DtypeWarning: Columns (4,5,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  icd_df = pd.read_csv('raw_data/icd-10-se.tsv', sep='\\t', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "# Load ICD-10-SE TSV file\n",
    "icd_df = pd.read_csv('raw_data/icd-10-se.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Show the first few rows\n",
    "icd_df.head()\n",
    "#print(icd_df[['Code', 'Title', 'Description']].head())\n",
    "\n",
    "column_renaming = {\n",
    "    \"Kod\": \"Code\",\n",
    "    \"Giltig fr.o.m.\": \"Valid From\",\n",
    "    \"Föräldrakod\": \"Parent Code\",\n",
    "    \"Benämning\": \"Title\",\n",
    "    \"Latin\": \"Latin\",\n",
    "    \"Beskrivning\": \"Description\",\n",
    "    \"Exempel\": \"Example\",\n",
    "    \"Innefattar\": \"Includes\",\n",
    "    \"Innefattar ej\": \"Excludes\",\n",
    "    \"Anmärkning\": \"Note\",\n",
    "    \"Kodningsinformation\": \"Coding Information\",\n",
    "    \"Innehåll\": \"Content\",\n",
    "    \"Manifestation (*)/ Etiologi (†)\": \"Manifestation (*)/ Etiology (†)\",\n",
    "    \"Länkad manifestation (*)/ etiologi (†)\": \"Linked Manifestation (*)/ Etiology (†)\",\n",
    "    \"Inte huvuddiagnos\": \"Not Primary Diagnosis\",\n",
    "    \"Kodnivå - kodspecifikation\": \"Code Level - Code Specification\"\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: This is the title of task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain the solution of task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should contain:\n",
    "- Results.\n",
    "- Summary of best model performance:\n",
    "    - Name of best model file as saved in /models.\n",
    "    - Relevant scores such as: accuracy, precision, recall, F1-score, etc.\n",
    "- Key discussion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use comments in the code to document specific steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER_Entity_Linking_Alberto_delosrios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

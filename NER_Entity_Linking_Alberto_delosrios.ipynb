{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC53jltwH0hL"
      },
      "source": [
        "# Assignment 1: Named Entity Recognition and Entity Linking Group Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHSZvTqtH0hM"
      },
      "source": [
        "Authors:  \n",
        "Alberto de los Ríos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgWLel79H0hN"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LgOWOGA8H0hN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IaP5-pUH0hO"
      },
      "outputs": [],
      "source": [
        "# It is recommended to start with general import statements\n",
        "#from utility_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRxqp6iwH0hP"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPB-Kh7NH0hP"
      },
      "source": [
        "This section should load the raw dataset for the task.  \n",
        "Remember to use relative paths to load any files in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "322rVEVbH0hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8ee6a1-df47-4085-a250-0da8c38fd75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "url_1177 = \"hf://datasets/community-datasets/swedish_medical_ner/1177/train-00000-of-00001.parquet\"\n",
        "url_lt = \"hf://datasets/community-datasets/swedish_medical_ner/lt/train-00000-of-00001.parquet\"\n",
        "url_wiki = \"hf://datasets/community-datasets/swedish_medical_ner/wiki/train-00000-of-00001.parquet\"\n",
        "\n",
        "df_1177 = pd.read_parquet(url_1177)\n",
        "#df_lt = pd.read_parquet(url_lt)\n",
        "df_wiki = pd.read_parquet(url_wiki)\n",
        "\n",
        "os.makedirs(\"raw_data\", exist_ok=True)\n",
        "df_1177.to_parquet(\"raw_data/1177_train.parquet\", engine=\"pyarrow\")\n",
        "#df_lt.to_parquet(\"raw_data/lt_train.parquet\", engine=\"pyarrow\")\n",
        "#df_wiki.to_parquet(\"raw_data/wiki_train.parquet\", engine=\"pyarrow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9FBZxYH0hP"
      },
      "source": [
        "## Task 1: LLMs for NER Survey\n",
        "\n",
        "This survey will be evalutated based on the following metrics and dimensions:\n",
        "\n",
        "- Language Capability (Swedish proficiency)\n",
        "- Biomedical Knowledge (Domain relevance)\n",
        "- Computational Efficiency (Training/inference costs)\n",
        "- Performance Metrics (NER-specific scores)\n",
        "- Ontology Compatibility (Linking to ICD/ICF/LOINC)\n",
        "\n",
        "PONER UN POCO DE HISTORIA DE BERT creado por los researches the Google\n",
        "\n",
        "For this first task we will only use the 1177 Vårdguiden dataset since it is the lightest with only 927 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkdsHI83H0hQ"
      },
      "source": [
        "# LLM in Swedish\n",
        "\n",
        "Advantages:\n",
        "1. Authenticity - Maintains original clinical nuance and terminology\n",
        "2. No translation errors - Avoids introducing errors from machine translation\n",
        "3. Proper noun handling - Swedish patient/place names and untranslatable terms remain intact\n",
        "4. Future applicability - Model will work natively with Swedish EHR systems\n",
        "5. Matching Swedish ontologies - Direct alignment with Swedish medical coding systems\n",
        "\n",
        "Limitations:\n",
        "1. Limited models - Fewer Swedish biomedical LLMs available:\n",
        "- KB/bert-base-swedish-cased: [Swedish BERT models for NER](https://huggingface.co/KB/bert-base-swedish-cased)\n",
        "- [RoBERTa large](https://huggingface.co/AI-Sweden-Models/roberta-large-1160k)\n",
        "- Swedish GPT models with limited NER capacity: [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
        "2. Smaller datasets - The Swedish medical NER dataset has only ~6,000 annotated entries\n",
        "3. Debugging difficulty - Hard to verify annotations/errors without Swedish knowledge\n",
        "4. Resource scarcity - Few Swedish stopword lists, tokenizers, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDc_tiRVH0hQ"
      },
      "source": [
        "# LLM in English\n",
        "\n",
        "- [Swedish BERT models for NER](https://huggingface.co/KB/bert-base-swedish-cased)\n",
        "- [RoBERTa large](https://huggingface.co/AI-Sweden-Models/roberta-large-1160k)\n",
        "- [AI Sweden Model Hub](https://huggingface.co/AI-Sweden-Models)\n",
        "\n",
        "\n",
        "Advantages:\n",
        "1. Model availability - Access powerful English biomedical models:\n",
        "- [BioBERT](https://github.com/naver/biobert-pretrained?tab=readme-ov-file)\n",
        "- [ClinicalBERT](https://huggingface.co/medicalai/ClinicalBERT)\n",
        "- [now BiomedBERT, previously known as PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext)\n",
        "2. Larger datasets - Can augment with English medical NER datasets (~20+ available)\n",
        "3. Easier debugging - You can understand the text for error analysis\n",
        "4. More tutorials - Abundant English NLP examples\n",
        "5. Ontology linking - English ontologies (ICD-10 English) have more community support\n",
        "\n",
        "Limitations:\n",
        "1. Translation errors - Clinical terms often mistranslated:\n",
        "2. Back-translation complexity - Need to map English predictions back to Swedish text\n",
        "3. Loss of context - Swedish compound words get split unnaturally\n",
        "4. Ontology mismatch - Swedish medical codes don't align perfectly with English\n",
        "Added pipeline complexity - Requires translation component\n",
        "\n",
        "- [BioBERT vs PubMedBERT](https://medium.com/@EleventhHourEnthusiast/model-comparison-biobert-vs-pubmedbert-8c2d78178d10)\n",
        "- [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pSXZNc_H0hQ"
      },
      "source": [
        "# Multilingual LLM\n",
        "\n",
        "- [mBERT](https://huggingface.co/google-bert/bert-base-multilingual-cased), 2018\n",
        "- [XLM-RoBERTa](https://huggingface.co/FacebookAI/xlm-roberta-base), 2019\n",
        "\n",
        "\n",
        "Advantages:\n",
        "1. Cross-Lingual Knowledge Transfer: Leverages patterns from high-resource languages (e.g., German medical terms help Swedish). Fine-tune on Swedish data but benefit from pretraining on multilingual medical corpora.\n",
        "2. Handling Code-Switching: No need for manual language detection.\n",
        "3. Robust Tokenization: SentencePiece (used in XLM-R) handles Swedish compounds better than WordPiece (mBERT):\n",
        "4. Future-Proofing: One model can support other languages (e.g., adding Norwegian EHRs later).\n",
        "\n",
        "Limitations:\n",
        "1. Translation errors - Clinical terms often mistranslated:\n",
        "2. Back-translation complexity - Need to map English predictions back to Swedish text\n",
        "3. Loss of context - Swedish compound words get split unnaturally\n",
        "4. Ontology mismatch - Swedish medical codes don't align perfectly with English\n",
        "Added pipeline complexity - Requires translation component\n",
        "\n",
        "- [BioBERT vs PubMedBERT](https://medium.com/@EleventhHourEnthusiast/model-comparison-biobert-vs-pubmedbert-8c2d78178d10)\n",
        "- [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_1177.head()"
      ],
      "metadata": {
        "id": "RNBFeAHgmlx7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse raw data"
      ],
      "metadata": {
        "id": "XzabzEKKyHbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "def analyse_df(df):\n",
        "\n",
        "  df.columns\n",
        "  print(f\"Total number of sentences: {len(df)}\")\n",
        "\n",
        "  # Count sentences with at least one entity\n",
        "  has_entity = df[\"entities\"].apply(lambda ent: len(ent[\"start\"]) > 0)\n",
        "  print(f\"Number of sentences with at least one entity: {has_entity.sum()}\")\n",
        "\n",
        "  from collections import Counter\n",
        "  # Initialize counter\n",
        "  type_counter = Counter()\n",
        "\n",
        "  # Loop through entities and count each type\n",
        "  for entity in df[\"entities\"]:\n",
        "      if \"type\" in entity:\n",
        "          types = entity[\"type\"]\n",
        "          if isinstance(types, np.ndarray) and types.size > 0:\n",
        "              type_counter.update(types.tolist())\n",
        "\n",
        "  # Print results\n",
        "  print(\"Entity counts from dataset:\")\n",
        "  print(f\"Disorder/Finding (type 0): {type_counter[0]}\")\n",
        "  print(f\"Pharmaceutical Drug (type 1): {type_counter[1]}\")\n",
        "  print(f\"Body Structure (type 2): {type_counter[2]}\")\n",
        "\n",
        "#analyse_df(df_1177)\n",
        "#analyse_df(df_lt)\n",
        "#analyse_df(df_wiki)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "F3wZAtz8Rft4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae4631e-d07a-4596-b7c8-bb83266d55ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This results do not match the specifications of the DataSet Summary where it states that we have 2740 annotations, out of which:\n",
        "\n",
        "- 1574 are disorder and findings (type 0)\n",
        "- 546 are pharmaceutical drug (type 1)\n",
        "- 620 are body structure. (type 2)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WdZue4xYMoCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max number of Tokens"
      ],
      "metadata": {
        "id": "eTImbvQWyNkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "max_tokens = 0\n",
        "\n",
        "for sent in df_1177[\"sentence\"]:\n",
        "    tokens = tokenizer(sent)[\"input_ids\"]\n",
        "    max_tokens = max(max_tokens, len(tokens))\n",
        "\n",
        "print(\"Maximum number of tokens:\", max_tokens)"
      ],
      "metadata": {
        "id": "U-edtrRQsSMF",
        "outputId": "e8487b78-1754-4d4c-9a77-03343665ccf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of tokens: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding and aligment of tokens and labels"
      ],
      "metadata": {
        "id": "JB1CQpd9yRoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer (multilingual, supports Swedish)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# BIO label mapping\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-DIS\": 1,\n",
        "    \"I-DIS\": 2,\n",
        "    \"B-DRUG\": 3,\n",
        "    \"I-DRUG\": 4,\n",
        "    \"B-BODY\": 5,\n",
        "    \"I-BODY\": 6\n",
        "}\n",
        "\n",
        "# Map integer entity types to BIO label prefixes\n",
        "type_to_bio_prefix = {\n",
        "    0: \"DIS\",\n",
        "    1: \"DRUG\",\n",
        "    2: \"BODY\"\n",
        "}\n",
        "\n",
        "def convert_to_bio_labels(encoding, entity_spans): #Converts character-level entity spans to BIO-formatted token-level labels.\n",
        "\n",
        "    labels = [label2id[\"O\"]] * len(encoding[\"input_ids\"])\n",
        "\n",
        "    for (ent_start, ent_end, ent_type) in entity_spans:\n",
        "        bio_label = type_to_bio_prefix[ent_type]\n",
        "        first_token = True\n",
        "\n",
        "        for idx, (tok_start, tok_end) in enumerate(encoding[\"offset_mapping\"]):\n",
        "            if tok_start == tok_end == 0:\n",
        "                continue  # Skip special tokens\n",
        "\n",
        "            if tok_end > ent_start and tok_start < ent_end:  # Token overlaps entity\n",
        "                if first_token:\n",
        "                    labels[idx] = label2id[f\"B-{bio_label}\"]\n",
        "                    first_token = False\n",
        "                else:\n",
        "                    labels[idx] = label2id[f\"I-{bio_label}\"]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def process_dataset(df, num_examples):\n",
        "    processed = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        row = df.iloc[i]\n",
        "        sentence = row[\"sentence\"]\n",
        "        entities = row[\"entities\"]  # expects keys: 'start', 'end', 'type'\n",
        "\n",
        "        encoding = tokenizer(\n",
        "            sentence,\n",
        "            return_offsets_mapping=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=80 #max_number of tokens for df_1177=72 80 to be safe. check the other df\n",
        "        )\n",
        "\n",
        "        # Prepare entity spans for BIO conversion\n",
        "        entity_spans = list(zip(entities[\"start\"], entities[\"end\"], entities[\"type\"]))\n",
        "        bio_labels = convert_to_bio_labels(encoding, entity_spans)\n",
        "        encoding[\"labels\"] = bio_labels\n",
        "\n",
        "        # We no longer need offset_mapping\n",
        "        encoding.pop(\"offset_mapping\")\n",
        "\n",
        "        if i < num_examples:\n",
        "          print(f\"\\nSentence {i+1}:\")\n",
        "          tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
        "          for token, label_id in zip(tokens, encoding[\"labels\"]):\n",
        "              label_name = list(label2id.keys())[list(label2id.values()).index(label_id)]\n",
        "              print(f\"{token:15} -> {label_name}\")\n",
        "\n",
        "        processed.append(encoding)\n",
        "\n",
        "    return processed\n",
        "\n",
        "    #print(\"\\nEncoding keys example:\", encoding.keys())\n",
        "\n",
        "\n",
        "encodings_df_1177 = process_dataset(df_1177, 3)\n",
        "torch.save(encodings_df_1177, \"processed_data/encodings_df_1177.pt\") #Save the processed data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0OJ-LR-SceT",
        "outputId": "1e4c21cd-fd00-4cdd-8a7a-43f397c81193"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1:\n",
            "<s>             -> O\n",
            "▁Mem            -> O\n",
            "ant             -> O\n",
            "in              -> O\n",
            "▁(              -> B-DIS\n",
            "▁Eb             -> I-DIS\n",
            "ixa             -> I-DIS\n",
            "▁)              -> I-DIS\n",
            "▁ger            -> O\n",
            "▁sällan         -> O\n",
            "▁några          -> O\n",
            "▁bi             -> O\n",
            "verk            -> O\n",
            "ningar          -> O\n",
            ".               -> O\n",
            "</s>            -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "\n",
            "Sentence 2:\n",
            "<s>             -> O\n",
            "▁Det            -> O\n",
            "▁är             -> O\n",
            "▁också          -> O\n",
            "▁lättare        -> O\n",
            "▁att            -> O\n",
            "▁dos            -> O\n",
            "era             -> O\n",
            "▁[              -> B-DRUG\n",
            "▁fly            -> I-DRUG\n",
            "tande           -> I-DRUG\n",
            "▁medicin        -> I-DRUG\n",
            "▁]              -> I-DRUG\n",
            "▁än             -> O\n",
            "▁att            -> O\n",
            "▁dela           -> O\n",
            "▁på             -> O\n",
            "▁tabletter      -> O\n",
            ".               -> O\n",
            "</s>            -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "\n",
            "Sentence 3:\n",
            "<s>             -> O\n",
            "▁(              -> B-DIS\n",
            "▁För            -> I-DIS\n",
            "stopp           -> I-DIS\n",
            "ning            -> I-DIS\n",
            "▁)              -> I-DIS\n",
            "▁är             -> O\n",
            "▁ett            -> O\n",
            "▁vanligt        -> O\n",
            "▁problem        -> O\n",
            "▁hos            -> O\n",
            "▁äldre          -> O\n",
            ".               -> O\n",
            "</s>            -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n",
            "<pad>           -> O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is already tokenized and shows the spans intances and types of labels.\n",
        "\n",
        "\n",
        " What you do need to do:\n",
        "1. Tokenize the sentence using a pretrained tokenizer, like BertTokenizerFast, which supports alignment of character offsets to token indices.\n",
        "\n",
        "2. Use tokenizer(..., return_offsets_mapping=True) to map character positions (like 'start' and 'end') to token indices.\n",
        "\n",
        "3. Create a labels list per sentence (same length as the number of tokens) initialized to -100 (special value that tells the model to ignore those positions), and then assign label IDs (0, 1, 2) to the tokens that overlap with entity spans.\n",
        "\n",
        "\n",
        "CUENTA TAMBIEN LOS CARACTERES ESPECIALES ({[ DE CADA CLASE ASIQ VALORAR SI NO CONTARLOS O SI. PERO SON INTICATIVOS DE LA TYPO DE LABEL.\n",
        "\n",
        "doubts:\n",
        "1. Why do all sentences need to be the same length (via padding)?\n",
        "Transformers process input as batches of fixed-size sequences. That’s because:\n",
        "\n",
        "Matrix computations (done on GPU) require tensors of uniform shape.\n",
        "\n",
        "Efficient batching boosts performance and stability during training.\n",
        "\n",
        "2. Explanation of common fields in encoding:\n",
        "- input_ids: The token IDs for your sentence (numbers representing tokens).\n",
        "- token_type_ids: Used for distinguishing sentence segments (mostly for tasks like question answering). Might be all zeros if not applicable.\n",
        "- attention_mask: Indicates which tokens are real (1) and which are padding (0).\n",
        "- offset_mapping: For each token, the start and end character positions in the original sentence (useful for aligning labels).\n",
        "\n",
        "3. Explicación:\n",
        "- entities[\"type\"] contains flat integers like 0, 1, 2, etc.\n",
        "- type_to_bio_prefix maps those integers to entity type names (DIS, DRUG, etc.)\n",
        "- convert_to_bio_labels() uses this mapping to apply B- and I- prefixes to the correct tokens.\n",
        "- Then it converts those prefixes using label2id.\n",
        "\n",
        "\n",
        "PONER EXPLICACIÓN DE BIO LABELS Y DE QUE MUCHOS MODELOS SOLO ACEPTAN ESTE TIPO DE LABELS.\n"
      ],
      "metadata": {
        "id": "J_gWYdvWRtdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE CUSTOM PYTORCH DATASET\n",
        "\n",
        "Why create a Dataset class if you already have encodings?\n",
        "- Integration with PyTorch DataLoader:\n",
        "The Dataset class provides a standardized way for PyTorch to access your data. The DataLoader uses the Dataset to efficiently load data in batches, shuffle it, and handle multi-threaded loading.\n",
        "\n",
        "- Batching & Shuffling:\n",
        "When training a model, you typically don’t want to feed data one example at a time. You want mini-batches (e.g., 16 or 32 samples per batch). The DataLoader uses the Dataset to create batches, and you don’t have to manually slice your encodings.\n",
        "\n",
        "- Lazy Access / Memory Efficiency:\n",
        "The Dataset class lets PyTorch load one sample at a time on demand, rather than keeping everything as tensors in memory at once (especially useful with large datasets).\n",
        "\n",
        "- Transforms / Augmentations:\n",
        "If you want to apply any transformations (like token masking, noise, data augmentation) on the fly, the Dataset class is the place to do it — at retrieval time.\n",
        "\n",
        "- Uniform Interface:\n",
        "The Dataset abstracts the data representation so your training loop just works with the Dataset/DataLoader API without worrying about how data is stored.\n",
        "\n",
        "- [Custom Named Entity Recognition with BERT](https://towardsdatascience.com/custom-named-entity-recognition-with-bert-cf1fd4510804/)"
      ],
      "metadata": {
        "id": "HNCdzCB4H6p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_encodings(processed):\n",
        "    encodings = {\"input_ids\": [], \"attention_mask\": []}\n",
        "    labels = []\n",
        "    for item in processed:\n",
        "        encodings[\"input_ids\"].append(item[\"input_ids\"])\n",
        "        encodings[\"attention_mask\"].append(item[\"attention_mask\"])\n",
        "        labels.append(item[\"labels\"])\n",
        "    return encodings, labels\n",
        "\n",
        "encodings, labels = collate_encodings(encodings_df_1177)"
      ],
      "metadata": {
        "id": "oLS9w7_nIdiH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # For each key (input_ids, attention_mask), convert the sequence at idx to tensor\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Also convert labels for that idx to tensor\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "tSd_Yd08JPrH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split and create the Dataset"
      ],
      "metadata": {
        "id": "zGPdzCEeMDUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate inputs and labels\n",
        "all_input_ids = [enc['input_ids'] for enc in encodings_df_1177]\n",
        "all_attention_mask = [enc['attention_mask'] for enc in encodings_df_1177]\n",
        "all_labels = [enc['labels'] for enc in encodings_df_1177]\n",
        "\n",
        "# Pack inputs into one dictionary for convenience\n",
        "all_encodings = {\n",
        "    'input_ids': all_input_ids,\n",
        "    'attention_mask': all_attention_mask\n",
        "}\n",
        "\n",
        "# Split\n",
        "\n",
        "train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_labels, val_labels = train_test_split(\n",
        "    all_input_ids, all_attention_mask, all_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Re-create encoding dicts for train and val\n",
        "train_encodings = {\n",
        "    'input_ids': train_input_ids,\n",
        "    'attention_mask': train_attention_mask\n",
        "}\n",
        "\n",
        "val_encodings = {\n",
        "    'input_ids': val_input_ids,\n",
        "    'attention_mask': val_attention_mask\n",
        "}\n",
        "\n",
        "# Now create datasets\n",
        "train_dataset = NERDataset(train_encodings, train_labels)\n",
        "val_dataset = NERDataset(val_encodings, val_labels)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wkFGEZzKJcCu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENE5CGWOH0hR"
      },
      "source": [
        "## Task 2: This is the title of task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJinuNA9H0hR"
      },
      "source": [
        "This section should contain the solution of task 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLvZVHzH0hS"
      },
      "source": [
        "## Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZYwaIDvH0hS"
      },
      "source": [
        "This section should contain:\n",
        "- Results.\n",
        "- Summary of best model performance:\n",
        "    - Name of best model file as saved in /models.\n",
        "    - Relevant scores such as: accuracy, precision, recall, F1-score, etc.\n",
        "- Key discussion points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qlEIll3H0hS"
      },
      "outputs": [],
      "source": [
        "# Always use comments in the code to document specific steps"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}